{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabfaruque/pythonintro/blob/main/traveling_covid_abstract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8MVamx0QAXg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def read_excel_data():\n",
        "\n",
        "    # Specify the columns we want to read\n",
        "    cols = [f\"t{i}_{var}\" for var in ['cesd_tot', 'lonely_tot', 'anx_fin', 'exerc'] for i in range(1, 13)] #SF variables edited\n",
        "\n",
        "    # Read the Excel file into a DataFrame, only reading the required columns\n",
        "    #df = pd.read_excel(\"C19CS_only100.xlsx\", usecols=cols)\n",
        "    df = pd.read_excel(\"C19CS_Data_Numeric.xlsx\", usecols=cols)\n",
        "\n",
        "    # Reshape each variable into a 2D array and store them in a list\n",
        "    variables = ['cesd_tot', 'lonely_tot', 'anx_fin', 'exerc'] #SF variables edited\n",
        "    matrices = []\n",
        "    for var in variables:\n",
        "        cols = [f\"t{i}_{var}\" for i in range(1, 13)]\n",
        "        matrix = df[cols].values,\n",
        "        matrices.append(matrix)\n",
        "    return matrices\n",
        "[cesd_tot, lonely_tot, anx_fin, exerc] = read_excel_data() #SF variables edited\n",
        "\n",
        "original_matrices = [cesd_tot, lonely_tot, anx_fin, exerc] #SF variables edited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-uEETipQAXh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_excel_data():\n",
        "    # Specify the columns we want to read\n",
        "    cols = [\"t0_age\", \"t0_sex\", \"t0_race_exp\", \"t0_relat\", \"t0_liv_a\", \"t0_educsn\", \"t0_cov_self\", \"t0_hhist_dep\", \"t0_hhist_anx\", \"t0_drink\",\"t0_cesd_fin\"] #added cesd_fin (0=nondepressed 1=depressed)\n",
        "\n",
        "    # Read the Excel file into a DataFrame, only reading the required columns\n",
        "    #df = pd.read_excel(\"C19CS_only100.xlsx\", usecols=cols)\n",
        "    df = pd.read_excel(\"C19CS_Data_Numeric.xlsx\", usecols=cols)\n",
        "    # Reshape each variable into a 2D array and store them in a list\n",
        "    matrix = df[cols].values,\n",
        "\n",
        "    return matrix\n",
        "\n",
        "M2 = read_excel_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhaQSQUeQAXh"
      },
      "outputs": [],
      "source": [
        "[cesd_tot, lonely_tot, anx_fin, exerc] = original_matrices #SF variables edited\n",
        "matrix_names = ['cesd_tot_clean', 'lonely_tot_clean', 'anx_fin_clean', 'exerc_clean'] #SF variables edited\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6Csh5h8QAXh"
      },
      "outputs": [],
      "source": [
        "# New code for displaying the sample sizes\n",
        "original_sample_sizes = [m[0].shape[0] for m in [cesd_tot, lonely_tot, anx_fin, exerc]]\n",
        "\n",
        "for i, size in enumerate(original_sample_sizes):\n",
        "    print(f'Sample size for {matrix_names[i]} before cleaning: {size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCnyyRCgQAXi"
      },
      "source": [
        "can you write a clean_data function that takes a list of 2D matrices, formatted like cesd_tot or lonely_tot above, and returns a new list of 2D matrices where any row that has a NAN in any of the 2D matrices is removed from all the 2D matrices.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hdYScmnQAXj"
      },
      "outputs": [],
      "source": [
        "# remove where there are any NANs, it's a lot\n",
        "import numpy as np\n",
        "ms = []\n",
        "nans = []\n",
        "nan_indices = nans\n",
        "new_matrices = []\n",
        "for m in original_matrices:\n",
        "        m2 = np.squeeze(m)\n",
        "        nans = np.where(np.any(np.isnan(m2), axis=1))[0]\n",
        "        nan_indices=np.rint(np.squeeze(np.union1d(nan_indices, nans))).astype(int)\n",
        "for m in original_matrices:\n",
        "        m2 = np.squeeze(m)\n",
        "        clean_m2 = np.delete(m2, nan_indices, axis=0)\n",
        "        new_matrices.append(clean_m2)\n",
        "\n",
        "M2_cleaned = np.delete(np.squeeze(M2), nan_indices, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsS70sS1QAXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DEQOEmAQAXk"
      },
      "outputs": [],
      "source": [
        "sample_sizes = [m.shape[0] for m in new_matrices]\n",
        "\n",
        "for i, sample_size in enumerate(sample_sizes):\n",
        "    print(f'Sample size for {matrix_names[i]} after removing NaNs: {sample_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF2IIjp-QAXl"
      },
      "source": [
        "write python code that will take the matrices variable matrices = [cesd_tot_clean, lonely_tot_clean, anx_fin_clean, worry_clean, satisfy_clean] = clean_data(original_matrices) and will output a new_matrices variable that has the same dimensions and same number of values, except it is normalized using standard fischer Z transform to put all the numbers in uniform normal distribution within dimension for the whole group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1LoyEcrQAXl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def normalize_matrices(matrices):\n",
        "    normalized_matrices = []\n",
        "    for matrix in matrices:\n",
        "        normalized_matrix = zscore(matrix, axis=0)  # Standardize along columns\n",
        "        normalized_matrices.append(normalized_matrix)\n",
        "    return normalized_matrices\n",
        "\n",
        "new2_matrices = normalize_matrices(new_matrices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw0ZquwOQAXl"
      },
      "outputs": [],
      "source": [
        "[cesd_tot_clean, lonely_tot_clean, anx_fin_clean, exerc_clean] = new2_matrices #SF variables edited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKnlWKbcQAXm"
      },
      "source": [
        "FIND PRIMARY COMPONENTS OF CHANGE WITH KMEANS:\n",
        "Instead of principal components that can be added together to explain the 5 vector change scores, I would like a larger set of components that best account for the variance independently.  For any time point we will be modeling with just the statistically closest components scaled to account for that change.  so I would like to have a lot of these independent components ordered by how much variance they account for in the data.  Can you do that?\n",
        "\n",
        "ChatGPT\n",
        "From your description, it sounds like you want to use a method such as k-means clustering to partition your dataset into a larger number of groups or \"components\", each representing a distinct pattern of changes in your variables. Each cluster centroid would then be a 5-dimensional vector representing a mode of behavior, and each data point could be represented as the centroid of the cluster to which it's closest.\n",
        "\n",
        "This is a different approach from PCA, which finds a smaller number of components that can be linearly combined to approximate the original dataset. Instead, k-means finds a set of representative points (the centroids) that partition the dataset into distinct clusters.\n",
        "\n",
        "Here is how you might do this with the k-means algorithm in the Scikit-Learn library:  (but first a cell getting the data structured correctly)\n",
        "\n",
        "...also write a python command that will a return 2D array output_array with array size num_subjects x time_points.  It will take as input change_array_3d which is a 3D array with array size num_subjects x num_time_points x num_dimensions. the command should iterate through the subjects in change_array_3d and fill the new 2D array  with the (11,) array output by use_viterbi using the 2D num_time_points x num_dimensions indexed in the iteration of subjects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h-tiuc_QAXm"
      },
      "outputs": [],
      "source": [
        "change_vectors = []\n",
        "\n",
        "# Compute change vectors for each matrix and store it in the list\n",
        "for matrix in new2_matrices:\n",
        "    # Compute the difference along the time axis (columns)\n",
        "    change_vector = np.diff(matrix, axis=1)\n",
        "    # Append the change_vector to the list\n",
        "    change_vectors.append(change_vector)\n",
        "\n",
        "#  ... came from somewhere else.   we may need the full group to get the k means better.  change_vector_matrix = change_vector(matrices)\n",
        "# Stack the change_vectors along the new axis (axis=2) to create a 3D array\n",
        "# This will have shape (num_subjects, num_timepoints, num_matrices)\n",
        "change_array_3d = np.stack(change_vectors, axis=2)\n",
        "\n",
        "# Reshape the 3D array into a 2D matrix\n",
        "# This will have shape (num_subjects * num_timepoints, num_matrices)\n",
        "change_vector_matrix = change_array_3d.reshape(-1, change_array_3d.shape[-1])\n",
        "\n",
        "def describe_matrix(matrix):\n",
        "    print(f\"Matrix Shape: {matrix.shape}\")\n",
        "    print(f\"Matrix Size: {matrix.size}\")\n",
        "    print(f\"Matrix Datatype: {matrix.dtype}\")\n",
        "    print(f\"Minimum Value in Matrix: {matrix.min()}\")\n",
        "    print(f\"Maximum Value in Matrix: {matrix.max()}\")\n",
        "    print(f\"Average Value in Matrix: {matrix.mean()}\")\n",
        "    print(f\"Standard Deviation of Values in Matrix: {matrix.std()}\")\n",
        "\n",
        "#\n",
        "# set some vars that get used later\n",
        "num_subjects = change_vector.shape[0]\n",
        "num_timepoints = change_vector.shape[1]\n",
        "subject_id = np.repeat(np.arange(1, num_subjects+1), num_timepoints)\n",
        "\n",
        "# Call a function to look at the change matrix\n",
        "describe_matrix(change_vector_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTKfrWusQAXm"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Determine the number of clusters you want\n",
        "num_clusters = 5 # let's try\n",
        "\n",
        "# Create the k-means object\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "\n",
        "# Fit the k-means object to the data\n",
        "kmeans.fit(change_vector_matrix)\n",
        "\n",
        "# The cluster centers are your components\n",
        "components = kmeans.cluster_centers_\n",
        "\n",
        "# You can find the closest component to each data point\n",
        "labels = kmeans.predict(change_vector_matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeaTdBZOQAXm"
      },
      "source": [
        "CREATE TRANSITION ARRAY AND LIKELIHOOD MATRIX, the likelihood matrix is what the HMM code expects, and it is the traditional transition matrix of HMM.  the tranisition array here is all the data and likelihood is the summarized.  Sorry for the added confusion. Chatgpt prompt: write a function called create_transition_matrix.  This function will take as input subject_id and labels.  These vectors have the same number of rows (timepts-1)*num_subjects.  This new matrix will have fewer rows.  It will have (timepts-2)*num_subjects. The first column in the new matrix should be labels with the final timept for each subject removed.  The second column should be the labels vector with the first row of each subject removed and then shifted up (i.e., what was row 2 is now row 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj1tDbh1QAXn"
      },
      "outputs": [],
      "source": [
        "def create_transition_matrix(subject_id, labels):\n",
        "    # unique subjects\n",
        "    unique_subjects = np.unique(subject_id)\n",
        "\n",
        "    # create empty list to store rows of new matrix\n",
        "    transition_matrix = []\n",
        "\n",
        "    # iterate over subjects\n",
        "    for subj in unique_subjects:\n",
        "        # get indices for the current subject\n",
        "        subj_indices = np.where(subject_id == subj)[0]\n",
        "\n",
        "        # get the labels for the current subject\n",
        "        subj_labels = labels[subj_indices]\n",
        "\n",
        "        # remove the last label for the current subject\n",
        "        col1 = subj_labels[:-1]\n",
        "\n",
        "        # remove the first label for the current subject\n",
        "        col2 = subj_labels[1:]\n",
        "\n",
        "        # append to the transition_matrix\n",
        "        transition_matrix.extend(list(zip(col1, col2)))\n",
        "\n",
        "    # convert list of tuples to numpy array\n",
        "    transition_matrix = np.array(transition_matrix)\n",
        "\n",
        "    return transition_matrix\n",
        "\n",
        "transition_matrix= create_transition_matrix(subject_id,labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV-eDkhzQAXn"
      },
      "source": [
        "Write a function that takes as input the transition matrix (transition_matrix) from the create_transition_matrix and ouputs a 2D matrix, for the transition likelihoods of from the label in the row to the label in the col.  The likelihhods can be calculated from the frequencies of the labels appearing in col 1 and col 2 in the transition_matrix.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1HzeYXpQAXn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_transition_likelihoods(transition_matrix):\n",
        "    # Number of unique labels (clusters)\n",
        "    num_labels = int(np.max(transition_matrix)) + 1\n",
        "\n",
        "    # Initialize the likelihood matrix with zeros\n",
        "    likelihood_matrix = np.zeros((num_labels, num_labels))\n",
        "\n",
        "    # Count transitions from one state to another\n",
        "    transition_counts = Counter(map(tuple, transition_matrix))\n",
        "\n",
        "    # Fill in the likelihood matrix\n",
        "    for (current_state, next_state), count in transition_counts.items():\n",
        "        likelihood_matrix[int(current_state), int(next_state)] = count\n",
        "\n",
        "    # Normalize rows to represent probabilities\n",
        "    row_sums = likelihood_matrix.sum(axis=1, keepdims=True)\n",
        "    likelihood_matrix = likelihood_matrix / row_sums\n",
        "\n",
        "    return likelihood_matrix\n",
        "\n",
        "likelihoods = calculate_transition_likelihoods(transition_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YakGfD22QAXn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def feature_values_in_clusters(k_means, feature_indices, feature_names):\n",
        "    n_clusters = k_means.n_clusters\n",
        "    feature_values = k_means.cluster_centers_\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    for i in range(n_clusters):\n",
        "        plt.subplot(n_clusters//2 + n_clusters%2, 2, i+1)\n",
        "        plt.barh(range(len(feature_indices)), feature_values[i, feature_indices], color='b', align='center')\n",
        "        plt.yticks(range(len(feature_indices)), [feature_names[idx] for idx in feature_indices])\n",
        "        plt.xlabel('Feature Value')\n",
        "        plt.title(f'Cluster {i+1}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Select features by indices\n",
        "feature_indices = [0, 1, 2, 3]  ##SF variables edited, removed the 4 # replace with your specific feature indices\n",
        "\n",
        "# Call the function\n",
        "feature_values_in_clusters(kmeans, feature_indices, matrix_names)\n",
        "\n",
        "# Now let's print out the cluster centers\n",
        "for i, center in enumerate(kmeans.cluster_centers_):\n",
        "    print(f\"Cluster {i+1}:\")\n",
        "    print(f\"  cesd_tot_clean: {center[0]:.2f}\")\n",
        "    print(f\"  lonely_tot_clean: {center[1]:.2f}\")\n",
        "    print(f\"  anx_fin_clean: {center[2]:.2f}\")\n",
        "    print(f\"  exerc_clean: {center[3]:.2f}\") #SF variables edited\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6EqnhgnQAXo"
      },
      "outputs": [],
      "source": [
        "#SF added, probability of each cluster being the first\n",
        "\n",
        "def calculate_initial_probabilities(subject_id, labels):\n",
        "    # Get the unique subjects\n",
        "    unique_subjects = np.unique(subject_id)\n",
        "\n",
        "    # Create an array to hold the count of each cluster appearing as the first state\n",
        "    initial_state_counts = np.zeros(num_clusters)  # Assuming num_clusters is the number of clusters\n",
        "\n",
        "    # Loop over each unique subject to find the first cluster label for that subject\n",
        "    for subj in unique_subjects:\n",
        "        # Get indices for the current subject\n",
        "        subj_indices = np.where(subject_id == subj)[0]\n",
        "\n",
        "        # Find the first label (cluster) for the current subject\n",
        "        first_label = labels[subj_indices[0]]\n",
        "\n",
        "        # Update the count for this label (cluster)\n",
        "        initial_state_counts[first_label] += 1\n",
        "\n",
        "    # Normalize the counts to get probabilities\n",
        "    total_subjects = len(unique_subjects)\n",
        "    initial_probabilities = initial_state_counts / total_subjects\n",
        "\n",
        "    return initial_probabilities\n",
        "\n",
        "# Calculate the initial probabilities\n",
        "initial_probabilities = calculate_initial_probabilities(subject_id, labels)\n",
        "\n",
        "print(\"Probability of each cluster being the first one [0,1,2,3,4]:\", initial_probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GC7k_8aQAXo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from hmmlearn import hmm\n",
        "import time\n",
        "n_states = 5\n",
        "\n",
        "def use_viterbi(observations):\n",
        "    model = hmm.GaussianHMM(n_components=n_states,  covariance_type=\"diag\")\n",
        "\n",
        "    model.initial_probabilities = np.array([0.17496962, 0.2235723  ,0.09842041 ,0.41919806 ,0.08383961])  #SF edit, previously 1/5 each... updated based on the output of the block above \"Probability of each cluster being the first one:\"\n",
        "    model.startprob_ = np.array([0.17496962, 0.2235723  ,0.09842041 ,0.41919806 ,0.08383961])  #SF edit, previously 1/5 each... updated based on the output of the block above \"Probability of each cluster being the first one:\"\n",
        "    model.transmat_ = likelihoods\n",
        "    model.means_ = components\n",
        "    model.covars_ = np.ones((5, 4)) #SF edited, previously: model.covars_ = np.ones((5, 5))\n",
        "    model.check_ = True\n",
        "    model.sklearn_is_fitted_ = True\n",
        "    predicted_states = model.predict(observations)\n",
        "    return  predicted_states\n",
        "\n",
        "def generate_output_array(change_array_3d):\n",
        "    # Get the number of subjects and time points from the input array\n",
        "    num_subjects, num_time_points, num_dimensions = change_array_3d.shape\n",
        "\n",
        "    # Initialize an empty output array\n",
        "    output_array = np.empty((num_subjects, num_time_points))\n",
        "\n",
        "    # Iterate through the subjects\n",
        "    for i in range(num_subjects):\n",
        "        # Get the data for the current subject\n",
        "        subject_data = change_array_3d[i]\n",
        "\n",
        "        # Use the use_viterbi function to generate the output for this subject\n",
        "        output_array[i] = use_viterbi(subject_data)\n",
        "\n",
        "    return output_array\n",
        "\n",
        "Covid_state_transitions = generate_output_array(change_array_3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAFjrTu0QAXo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming covid_state_transitions is an ndarray of shape (121, 11)\n",
        "# Generate a sample covid_state_transitions for demonstration\n",
        "\n",
        "# Initialize an empty array of zeros for the histogram with shape (121, 5)\n",
        "covid_histogram = np.zeros((823, 5), dtype=int)\n",
        "\n",
        "# Loop through each row in covid_state_transitions\n",
        "for row in range(0,823):\n",
        "    Current_row = Covid_state_transitions[row,:]\n",
        "    covid_histogram[row,0] = np.count_nonzero(Current_row == 0)\n",
        "    covid_histogram[row,1] = np.count_nonzero(Current_row == 1)\n",
        "    covid_histogram[row,2] = np.count_nonzero(Current_row == 2)\n",
        "    covid_histogram[row,3] = np.count_nonzero(Current_row == 3)\n",
        "    covid_histogram[row,4] = np.count_nonzero(Current_row == 4)\n",
        "\n",
        "\n",
        "# covid_histogram should now have the count of occurrences of each number (0 to 4) for each row.\n",
        "print(\"Covid Histogram:\")\n",
        "print(covid_histogram)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh56y2fTQAXp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you've already extracted the depressed and non-depressed sub-groups:\n",
        "t0_cesd_fin = M2_cleaned[:, -1]  # assuming t0_cesd_fin is the last column in M2_cleaned\n",
        "depressed_indices = np.where(t0_cesd_fin == 1)[0]\n",
        "non_depressed_indices = np.where(t0_cesd_fin == 0)[0]\n",
        "\n",
        "covid_histogram_depressed = covid_histogram[depressed_indices]\n",
        "covid_histogram_non_depressed = covid_histogram[non_depressed_indices]\n",
        "\n",
        "# Calculate the mean for each column in the two sub-groups:\n",
        "mean_depressed = np.mean(covid_histogram_depressed, axis=0)\n",
        "mean_non_depressed = np.mean(covid_histogram_non_depressed, axis=0)\n",
        "\n",
        "# Plotting\n",
        "columns = ['0', '1', '2', '3', '4']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(columns))\n",
        "\n",
        "bar1 = plt.bar(index, mean_depressed, bar_width, color='b', label='Depressed')\n",
        "bar2 = plt.bar(index + bar_width, mean_non_depressed, bar_width, color='r', label='Non-Depressed')\n",
        "\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Mean Frequency')\n",
        "plt.title('Comparison of Mean Frequencies Between Depressed and Non-Depressed')\n",
        "plt.xticks(index + bar_width / 2, columns)  # Positioning the labels in the middle of the combined bars\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lph9QkyGQAXp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Assuming you've already extracted the depressed and non-depressed sub-groups:\n",
        "t0_cesd_fin = M2_cleaned[:, -1]  # assuming t0_cesd_fin is the last column in M2_cleaned\n",
        "depressed_indices = np.where(t0_cesd_fin == 1)[0]\n",
        "non_depressed_indices = np.where(t0_cesd_fin == 0)[0]\n",
        "\n",
        "covid_histogram_depressed = covid_histogram[depressed_indices]\n",
        "covid_histogram_non_depressed = covid_histogram[non_depressed_indices]\n",
        "\n",
        "# Calculate the mean, standard deviation, and sample size for each column in the two sub-groups:\n",
        "mean_depressed = np.mean(covid_histogram_depressed, axis=0)\n",
        "std_depressed = np.std(covid_histogram_depressed, axis=0, ddof=1)\n",
        "n_depressed = len(depressed_indices)\n",
        "\n",
        "mean_non_depressed = np.mean(covid_histogram_non_depressed, axis=0)\n",
        "std_non_depressed = np.std(covid_histogram_non_depressed, axis=0, ddof=1)\n",
        "n_non_depressed = len(non_depressed_indices)\n",
        "\n",
        "# Display means, stds, and sample sizes\n",
        "columns = ['0', '1', '2', '3', '4']\n",
        "for i, col in enumerate(columns):\n",
        "    print(f\"Column {col}:\")\n",
        "    print(f\"  Depressed - Mean: {mean_depressed[i]:.2f}, Std: {std_depressed[i]:.2f}, N: {n_depressed}\")\n",
        "    print(f\"  Non-Depressed - Mean: {mean_non_depressed[i]:.2f}, Std: {std_non_depressed[i]:.2f}, N: {n_non_depressed}\")\n",
        "\n",
        "    # Conduct t-test\n",
        "    t_stat, p_val = ttest_ind(covid_histogram_depressed[:, i], covid_histogram_non_depressed[:, i], equal_var=False)\n",
        "    print(f\"  t-test: t = {t_stat:.2f}, p = {p_val:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGvR_cBTQAXp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you've already extracted the demographic info:\n",
        "t0_cesd_fin = M2_cleaned[:, -1]  # assuming t0_cesd_fin is the last column in M2_cleaned\n",
        "t0_sex = M2_cleaned[:, 1]  # assuming t0_sex is the second column in M2_cleaned\n",
        "\n",
        "columns = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# Define a function to generate the plots\n",
        "def plot_by_group(depressed_data, non_depressed_data, group_name):\n",
        "    mean_depressed = np.mean(depressed_data, axis=0)\n",
        "    mean_non_depressed = np.mean(non_depressed_data, axis=0)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bar_width = 0.35\n",
        "    index = np.arange(len(columns))\n",
        "\n",
        "    plt.bar(index, mean_depressed, bar_width, color='b', label='Depressed')\n",
        "    plt.bar(index + bar_width, mean_non_depressed, bar_width, color='r', label='Non-Depressed')\n",
        "\n",
        "    plt.xlabel('Columns')\n",
        "    plt.ylabel('Mean Frequency')\n",
        "    plt.title(f'Comparison of Mean Frequencies Between Depressed and Non-Depressed ({group_name})')\n",
        "    plt.xticks(index + bar_width / 2, columns)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Stratify by gender\n",
        "for gender in [0, 1]:  # assuming 0 and 1 are the codes for the two genders\n",
        "    gender_indices = np.where(t0_sex == gender)[0]\n",
        "\n",
        "    depressed_by_gender = covid_histogram[np.intersect1d(depressed_indices, gender_indices)]\n",
        "    non_depressed_by_gender = covid_histogram[np.intersect1d(non_depressed_indices, gender_indices)]\n",
        "\n",
        "    plot_by_group(depressed_by_gender, non_depressed_by_gender, f\"Gender {gender}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vV5BuJzQAXq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Assuming you've already extracted the demographic info:\n",
        "t0_cesd_fin = M2_cleaned[:, -1]\n",
        "t0_sex = M2_cleaned[:, 1]\n",
        "\n",
        "columns = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# Define a function to compute and display statistics\n",
        "def compute_statistics(depressed_data, non_depressed_data, group_name):\n",
        "    t_values = []\n",
        "    p_values = []\n",
        "\n",
        "    mean_depressed = np.mean(depressed_data, axis=0)\n",
        "    std_depressed = np.std(depressed_data, axis=0)\n",
        "    size_depressed = len(depressed_data)\n",
        "\n",
        "    mean_non_depressed = np.mean(non_depressed_data, axis=0)\n",
        "    std_non_depressed = np.std(non_depressed_data, axis=0)\n",
        "    size_non_depressed = len(non_depressed_data)\n",
        "\n",
        "    for i in range(depressed_data.shape[1]):\n",
        "        t_val, p_val = ttest_ind(depressed_data[:, i], non_depressed_data[:, i])\n",
        "        t_values.append(t_val)\n",
        "        p_values.append(p_val)\n",
        "\n",
        "    print(f\"------ {group_name} ------\")\n",
        "    for col, t, p, mean_d, std_d, mean_nd, std_nd in zip(columns, t_values, p_values,\n",
        "                                                         mean_depressed, std_depressed,\n",
        "                                                         mean_non_depressed, std_non_depressed):\n",
        "        print(f\"Column {col}:\")\n",
        "        print(f\"  Depressed: mean={mean_d:.2f}, std={std_d:.2f}, n={size_depressed}\")\n",
        "        print(f\"  Non-Depressed: mean={mean_nd:.2f}, std={std_nd:.2f}, n={size_non_depressed}\")\n",
        "        print(f\"  t={t:.2f}, p={p:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Stratify by gender and compute statistics\n",
        "for gender in [0, 1]:\n",
        "    gender_indices = np.where(t0_sex == gender)[0]\n",
        "\n",
        "    depressed_by_gender = covid_histogram[np.intersect1d(depressed_indices, gender_indices)]\n",
        "    non_depressed_by_gender = covid_histogram[np.intersect1d(non_depressed_indices, gender_indices)]\n",
        "\n",
        "    compute_statistics(depressed_by_gender, non_depressed_by_gender, f\"Gender {gender}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0GUDmZ1QAXq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Assuming you've already extracted the demographic info:\n",
        "t0_cesd_fin = M2_cleaned[:, -1]\n",
        "t0_sex = M2_cleaned[:, 1]\n",
        "columns = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# 1. Main Effect of Gender:\n",
        "print(\"------ Main Effect of Gender ------\")\n",
        "men_indices = np.where(t0_sex == 0)[0]\n",
        "women_indices = np.where(t0_sex == 1)[0]\n",
        "\n",
        "men_data = covid_histogram[men_indices]\n",
        "women_data = covid_histogram[women_indices]\n",
        "\n",
        "for col in range(men_data.shape[1]):\n",
        "    t_val, p_val = ttest_ind(men_data[:, col], women_data[:, col])\n",
        "    print(f\"Column {columns[col]}: t={t_val:.2f}, p={p_val:.4f}\")\n",
        "\n",
        "# 2. Interaction Effect of Depression x Gender:\n",
        "print(\"\\n------ Interaction Effect of Depression x Gender ------\")\n",
        "for gender, name in [(0, \"Men\"), (1, \"Women\")]:\n",
        "    gender_indices = np.where(t0_sex == gender)[0]\n",
        "\n",
        "    depressed_by_gender = covid_histogram[np.intersect1d(depressed_indices, gender_indices)]\n",
        "    non_depressed_by_gender = covid_histogram[np.intersect1d(non_depressed_indices, gender_indices)]\n",
        "\n",
        "    print(f\"\\nFor {name}:\")\n",
        "    for col in range(depressed_by_gender.shape[1]):\n",
        "        t_val, p_val = ttest_ind(depressed_by_gender[:, col], non_depressed_by_gender[:, col])\n",
        "        print(f\"Column {columns[col]}: t={t_val:.2f}, p={p_val:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}